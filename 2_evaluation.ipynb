{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cybersecurity RAG Evaluation Notebook\n",
    "\n",
    "This notebook allows for an interactive evaluation of the RAG pipeline. It performs the same function as `scripts/run_evaluation.py` but provides a step-by-step execution with visible logs and outputs, including a confusion matrix chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Add src to the Python path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from src.config import Config\n",
    "from src.rag_pipeline.graph import build_rag_graph\n",
    "from src.evaluation.evaluator import evaluate_performance\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "logging.info(\"Libraries and configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for API key before running\n",
    "if not Config.GROQ_API_KEY:\n",
    "    logging.error(\"GROQ_API_KEY is not set in the .env file. Evaluation cannot proceed.\")\n",
    "elif not Config.MONGO_URI:\n",
    "    logging.error(\"MONGO_URI is not set in the .env file. Evaluation cannot proceed.\")\n",
    "else:\n",
    "    # Load a sample of the eval dataset\n",
    "    try:\n",
    "        eval_df = pd.read_csv(Config.EVAL_DATA_PATH)\n",
    "        # Use a small sample for quick testing, you can increase the fraction\n",
    "        sample_df = eval_df.sample(frac=0.1, random_state=42)\n",
    "        logging.info(f\"Loaded {len(sample_df)} samples from {Config.EVAL_DATA_PATH}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Evaluation file not found at {Config.EVAL_DATA_PATH}\")\n",
    "        sample_df = None\n",
    "\n",
    "    if sample_df is not None:\n",
    "        app = build_rag_graph()\n",
    "        \n",
    "        questions = []\n",
    "        generated_answers = []\n",
    "        ground_truths = []\n",
    "        choices_list = []\n",
    "\n",
    "        logging.info(f\"Running evaluation on {len(sample_df)} samples...\")\n",
    "        # Use iterrows() for DataFrame iteration in a notebook for clarity\n",
    "        for index, row in sample_df.iterrows():\n",
    "            logging.info(f\"Processing sample {index+1}/{len(sample_df)}: Question ID {row.name}\")\n",
    "            \n",
    "            question = row[\"question\"]\n",
    "            ground_truth_choice = row[\"answer\"]\n",
    "            choices = row[\"choices\"]\n",
    "            \n",
    "            # The input to the graph is a dictionary with keys matching the RAGState\n",
    "            inputs = {\"query\": question, \"conversation_history\": \"\"}\n",
    "            result = app.invoke(inputs)\n",
    "            \n",
    "            questions.append(question)\n",
    "            generated_answers.append(result[\"answer\"])\n",
    "            ground_truths.append(ground_truth_choice)\n",
    "            choices_list.append(choices)\n",
    "\n",
    "        # Run the final evaluation and print the report\n",
    "        evaluate_performance(questions, generated_answers, ground_truths, choices_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
